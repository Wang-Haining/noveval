
## wikipedia_en dataset

after running `prepare.py` (preprocess) we get:

- train.bin is ~8.7GB, val.bin ~4.3MB
- train has ~4.6B tokens (4,645,199,244)
- val has ~2.2M tokens (2,231,052)

this came from 6,458,670 documents in total.

references:

- https://huggingface.co/datasets/wikipedia
